{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "04_Naive_Bayes_Trabalho_Helio_396446.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udu7xgdNu76k"
      },
      "source": [
        "### UFC, CC, DC, BCC (2019.1), Mineração de Dados (CK0223), Prof.: J. Macedo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd-_VGUNu76s"
      },
      "source": [
        "# Naive Bayes - Trabalho\n",
        "\n",
        "**Equipe**\n",
        "- Caio Rodrigues Costa (384345)\n",
        "\n",
        "- Hélio Henrique Barbosa Rocha (396446)\n",
        "\n",
        "- Vando Aderson Sacramento Alves (343177)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx8rqJJou76t"
      },
      "source": [
        "## Questão 1\n",
        "\n",
        "Implemente um classifacor Naive Bayes para o problema de predizer a qualidade de um carro. Para este fim, utilizaremos um conjunto de dados referente a qualidade de carros, disponível no [UCI](https://archive.ics.uci.edu/ml/datasets/car+evaluation). Este dataset de carros possui as seguintes features e classe:\n",
        "\n",
        "**Attributos**\n",
        "1. buying: vhigh, high, med, low\n",
        "2. maint: vhigh, high, med, low\n",
        "3. doors: 2, 3, 4, 5, more\n",
        "4. persons: 2, 4, more\n",
        "5. lug_boot: small, med, big\n",
        "6. safety: low, med, high\n",
        "\n",
        "**Classes**\n",
        "1. unacc, acc, good, vgood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NkyXiSu76u"
      },
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fhOERa0u76u"
      },
      "source": [
        "def isint(s):\n",
        "    try:\n",
        "        int(s)\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def loadCsv(filename):\n",
        "    lines = csv.reader(open(filename, 'r'))\n",
        "    dataset = list(lines)\n",
        "    for i in range(len(dataset)):\n",
        "        for j in range(len(dataset[0])):\n",
        "            if isint(dataset[i][j]):\n",
        "                dataset[i][j] = float(dataset[i][j])\n",
        "            else:\n",
        "                if dataset[i][j] == 'vhigh' or dataset[i][j] == 'vgood':\n",
        "                    dataset[i][j] = float(4)\n",
        "                elif dataset[i][j] == 'high' or dataset[i][j] == 'big' \\\n",
        "                    or dataset[i][j] == 'good':\n",
        "                    dataset[i][j] = float(3)\n",
        "                elif dataset[i][j] == 'med' or dataset[i][j] == 'acc':\n",
        "                    dataset[i][j] = float(2)\n",
        "                elif dataset[i][j] == 'low' or dataset[i][j] == 'small' \\\n",
        "                    or dataset[i][j] == 'unacc':\n",
        "                    dataset[i][j] = float(1)\n",
        "                elif dataset[i][j] == '5more':\n",
        "                    dataset[i][j] = float(5)\n",
        "                elif dataset[i][j] == 'more':\n",
        "                    dataset[i][j] = float(6)\n",
        "    return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WKygm4yu76w"
      },
      "source": [
        "filename = 'carData.csv'\n",
        "dataset = loadCsv(filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VttlZjBOu76x"
      },
      "source": [
        "cars = {\n",
        "    'data': np.array([(dataset[i])[0:6] for i in range(len(dataset))]),\n",
        "    'target': np.array([dataset[i][-1] for i in range(len(dataset))]),\n",
        "    'DESCR': '',\n",
        "    'feature_names': 'buying maint doors persons lug_boot safety',\n",
        "    'data_filename': '',\n",
        "    'target_filename': '',\n",
        "}\n",
        "\n",
        "cols = cars['feature_names'].split()\n",
        "X, y = cars['data'], cars['target']\n",
        "df = pd.DataFrame(X, columns=cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqP7fDbFu76x"
      },
      "source": [
        "def splitDataset(dataset, splitRatio):\n",
        "    trainSize = int(len(dataset) * splitRatio)\n",
        "    trainSet = []\n",
        "    copy = list(dataset)\n",
        "    while len(trainSet) < trainSize:\n",
        "        index = random.randrange(len(copy))\n",
        "        trainSet.append(copy.pop(index))\n",
        "    return [trainSet, copy]\n",
        "\n",
        "\n",
        "def separateByClass(dataset):\n",
        "    separated = {}\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        if vector[-1] not in separated:\n",
        "            separated[vector[-1]] = []\n",
        "        separated[vector[-1]].append(vector)\n",
        "    return separated\n",
        "\n",
        "\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([pow(x - avg, 2) for x in numbers]) \\\n",
        "        / float(len(numbers) - 1)\n",
        "    return math.sqrt(variance)\n",
        "\n",
        "\n",
        "def summarize(dataset):\n",
        "    summaries = [(mean(attribute), stdev(attribute)) for attribute in\n",
        "                 zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "\n",
        "def summarizeByClass(dataset):\n",
        "    separated = separateByClass(dataset)\n",
        "    summaries = {}\n",
        "    for (classValue, instances) in separated.items():\n",
        "        summaries[classValue] = summarize(instances)\n",
        "    return summaries\n",
        "\n",
        "\n",
        "def calculateProbability(x, mean, stdev):\n",
        "    try:\n",
        "        exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2))))\n",
        "    except ZeroDivisionError:\n",
        "        exponent = 0\n",
        "        \n",
        "    try:\n",
        "        z = 1 / (math.sqrt(2 * math.pi) * stdev) * exponent \n",
        "    except ZeroDivisionError:\n",
        "        z = 0\n",
        "    \n",
        "    return z\n",
        "\n",
        "\n",
        "def calculateClassProbabilities(summaries, inputVector):\n",
        "    probabilities = {}\n",
        "    for (classValue, classSummaries) in summaries.items():\n",
        "        probabilities[classValue] = 1\n",
        "        for i in range(len(classSummaries)):\n",
        "            (mean, stdev) = classSummaries[i]\n",
        "            x = inputVector[i]\n",
        "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def predict(summaries, inputVector):\n",
        "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
        "    (bestLabel, bestProb) = (None, -1)\n",
        "    for (classValue, probability) in probabilities.items():\n",
        "        if bestLabel is None or probability > bestProb:\n",
        "            bestProb = probability\n",
        "            bestLabel = classValue\n",
        "    return bestLabel\n",
        "\n",
        "\n",
        "def getPredictions(summaries, testSet):\n",
        "    predictions = []\n",
        "    for i in range(len(testSet)):\n",
        "        result = predict(summaries, testSet[i])\n",
        "        predictions.append(result)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def getAccuracy(testSet, predictions):\n",
        "    correct = 0\n",
        "    for i in range(len(testSet)):\n",
        "        if testSet[i][-1] == predictions[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(testSet)) * 100.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnRAqyUwu76z"
      },
      "source": [
        "# criacao das variaveis de train e test, particionamento de dados]\n",
        "splitRatio = 0.2\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X, \n",
        "                                                      y, \n",
        "                                                      test_size=splitRatio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR7dgSOgu76z"
      },
      "source": [
        "## Questão 2\n",
        "Crie uma versão de sua implementação usando as funções disponíveis na biblioteca SciKitLearn para o Naive Bayes ([veja aqui](http://scikit-learn.org/stable/modules/naive_bayes.html)) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnIiugPMu760"
      },
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEKJCeeQu761",
        "outputId": "e3d93e30-59ac-4cb2-a22e-78c49a7ce4d1"
      },
      "source": [
        "# Construcao do modelo Naive-Bayes Multinomial\n",
        "classificadorMNB = MultinomialNB()\n",
        "\n",
        "modelo = classificadorMNB.fit(X_train, y_train)\n",
        "predicoes = classificadorMNB.predict(X_test)\n",
        "\n",
        "result = modelo.score(X_test, y_test)\n",
        "\n",
        "# Aplicando 'k-Fold Cross Validation'\n",
        "acc = cross_val_score(estimator=classificadorMNB, \n",
        "                      X=X, \n",
        "                      y=y, \n",
        "                      cv=10)\n",
        "\n",
        "print ('Acurácia Naive-Bayes Multinomial (sciKitLearn):')\n",
        "print ('-------------------------------------------------')\n",
        "print ('Score   \\t| Cross-Validation')\n",
        "print ('-------------------------------------------------')\n",
        "print ('{:.2%}  \\t| {:.2%} +/-{:.2%}'.format(result, acc.mean(), acc.std()))\n",
        "print ('-------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia Naive-Bayes Multinomial (sciKitLearn):\n",
            "-------------------------------------------------\n",
            "Score   \t| Cross-Validation\n",
            "-------------------------------------------------\n",
            "68.50%  \t| 69.85% +/-0.35%\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDEwDU2Bu765"
      },
      "source": [
        "## Questão 3\n",
        "\n",
        "Analise a acurácia dos dois algoritmos e discuta a sua solução."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBx2LFgvu765"
      },
      "source": [
        "def tset(X, y):\n",
        "    t_set = [np.append(X[i], y[i]) for i in range(len(X))]\n",
        "    \n",
        "    return t_set\n",
        "\n",
        "\n",
        "# mesmos dados sao utilizados para calculos nos modelos das questoes 1 e 2\n",
        "trainingSet, testSet = tset(X_train, y_train), tset(X_test, y_test)\n",
        "\n",
        "\n",
        "(trainingSet, testSet) = splitDataset(dataset, splitRatio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNhGiXru766",
        "outputId": "77f2e74c-703f-49a1-d4e5-20039d8dc63b"
      },
      "source": [
        "def naiveBayes():\n",
        "    # preparar modelo\n",
        "    summaries = summarizeByClass(trainingSet)\n",
        "\n",
        "    # testar modelo\n",
        "    predictions = getPredictions(summaries, testSet)\n",
        "    accuracy = getAccuracy(testSet, predictions)\n",
        "    print ('Acurácia Naive-Bayes Multinomial:')\n",
        "    print ('-----------------------------------')\n",
        "    print ('Score')\n",
        "    print ('-----------------------------------')\n",
        "    print ('{:.2f}%'.format(accuracy))\n",
        "    print ('-----------------------------------')\n",
        "\n",
        "\n",
        "naiveBayes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia Naive-Bayes Multinomial:\n",
            "-----------------------------------\n",
            "Score\n",
            "-----------------------------------\n",
            "77.95%\n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbm0IS3u766"
      },
      "source": [
        "### Discusão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX0YB0VYu767"
      },
      "source": [
        "Conforme resultados observados, pode-se considerar que os algoritmos empregados neste trabalho são eficazes. Para algumas situações, a despeito de determinados presupostos, a classificação mostra-se apropriada. Ainda assim, trata-se de um algoritmo altamente dependente da frequência relativa de cada classe. Dado que a probabilidade de verossimilhança é proporcional à probabilidade marginal das classes, aquelas que possuem uma frequência muito maior em relação as demais terão uma probabilidade maior, e impactarão no resultado da probabilidade final. Por isso, as classes com menores ocorrências acabam não tendo um desempenho satisfatório. Isto é ainda mais evidente no método implementado pelo sciKitLearn. Por utilizar diferentes métodos de contagem, ainda que possua um melhor custo computacional, ele acaba subestimando as probabilidades das classes menos frequentes e, por consequência, tem-se que as predições caem quase que exclusivamente nas classes maior frequência."
      ]
    }
  ]
}